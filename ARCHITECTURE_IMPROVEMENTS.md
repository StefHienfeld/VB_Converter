# ğŸ—ï¸ Architecture Improvements - Implementation Plan

**Focus:** Scalability, Performance, Production Readiness
**Timeline:** 2-3 weeks (incremental deployment)
**Risk Level:** ğŸŸ¡ MEDIUM (backwards compatible, requires new dependencies)

---

## ğŸ“Š Current Architecture Problems

### Problem 1: In-Memory Job Storage âš ï¸

```python
# hienfeld_api/app.py line 116
JOBS: Dict[str, AnalysisJob] = {}  # âŒ Lost on restart!
```

**Impact:**
- âŒ Jobs lost when server restarts
- âŒ Can't scale horizontally (multiple instances)
- âŒ No persistence for audit trails
- âŒ Can't resume failed jobs

**Example Failure Scenario:**
```
User starts 25-minute analysis...
[15 minutes pass]
Server crashes / deploys new version
â†’ All progress lost! User has to restart ğŸ˜¡
```

---

### Problem 2: Models Reloaded Every Analysis ğŸŒ

```python
# Every analysis request:
nlp_service = NLPService(config)  # âŒ Loads 200MB SpaCy model!
embeddings_service = create_embeddings_service(...)  # âŒ Loads 90MB embedding model!
tfidf_service = DocumentSimilarityService(config)  # âŒ Trains from scratch!
```

**Impact:**
- ğŸŒ **First request:** 30-60 seconds just loading models
- ğŸŒ **Second request:** Another 30-60 seconds (not cached!)
- ğŸ’¾ Memory: 4 analyses = 4Ã— models in memory (1.2GB!)

**Expected Impact of Caching:**
- âœ… First request: 60s (same)
- âœ… Second+ requests: **2-3 seconds** (just retrieves cached instances)
- âœ… Memory: 1Ã— models (300MB), shared across all requests
- ğŸš€ **50-70% faster** repeat analyses

---

### Problem 3: No Task Queue ğŸ“¦

```python
# hienfeld_api/app.py line 736
background_tasks.add_task(_run_analysis_job, ...)  # âŒ No retry, no monitoring
```

**Impact:**
- âŒ No retry on failure
- âŒ Can't prioritize jobs
- âŒ No distributed processing
- âŒ Limited visibility (where is job in queue?)

---

### Problem 4: Minimal Monitoring ğŸ“ˆ

**Current:**
- Basic health endpoint (`/api/health` returns `{"status": "ok"}`)
- Logs only
- No metrics

**Missing:**
- âŒ Response time tracking
- âŒ Error rate monitoring
- âŒ Queue depth visibility
- âŒ Resource usage (CPU, memory)
- âŒ Analysis throughput metrics

---

## ğŸ¯ Improvement Roadmap

### Phase 1: Model Caching (Week 1) ğŸš€
**Priority:** ğŸ”´ HIGHEST
**Effort:** 2-3 days
**Impact:** 50-70% faster, 80% less memory

**Why First?**
- Biggest performance win
- Low risk (internal refactor)
- No new infrastructure needed
- Immediate user value

---

### Phase 2: Redis Job Storage (Week 2) ğŸ’¾
**Priority:** ğŸŸ¡ HIGH
**Effort:** 3-4 days
**Impact:** Persistent state, horizontal scaling

**Why Second?**
- Enables production deployment
- Prevents data loss
- Foundation for distributed architecture

---

### Phase 3: Enhanced Monitoring (Week 2-3) ğŸ“Š
**Priority:** ğŸŸ¡ HIGH
**Effort:** 2 days
**Impact:** Better observability, faster debugging

**Why Third?**
- Essential for production
- Helps identify bottlenecks
- Enables proactive alerts

---

### Phase 4: Celery Task Queue (Week 3+) ğŸ“¦
**Priority:** ğŸŸ¢ MEDIUM
**Effort:** 4-5 days
**Impact:** Distributed processing, retry logic

**Why Last?**
- Most complex
- Requires Redis (Phase 2 first)
- Nice-to-have vs must-have
- Can defer if needed

---

## ğŸ“‹ Phase 1: Model Caching Implementation

### Overview

**Goal:** Share loaded models across all analysis requests instead of reloading every time.

**Pattern:** Singleton service instances with lazy loading

---

### Step 1: Create Service Cache Manager

**File:** `hienfeld/services/service_cache.py` (NEW)

```python
"""
Service cache manager for sharing expensive model instances.

This prevents reloading heavy models (SpaCy, embeddings, TF-IDF) on every request.
Models are loaded once and cached globally.
"""

from __future__ import annotations

import threading
from typing import Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime

from hienfeld.logging_config import get_logger

logger = get_logger('service_cache')


@dataclass
class CacheEntry:
    """Metadata for a cached service instance"""
    service: Any
    created_at: datetime
    access_count: int
    last_accessed: datetime


class ServiceCache:
    """
    Thread-safe singleton cache for expensive service instances.

    Usage:
        cache = ServiceCache.get_instance()

        # Get or create NLP service
        nlp = cache.get_or_create(
            'nlp_service',
            lambda: NLPService(config),
            ttl=3600  # Cache for 1 hour
        )
    """

    _instance: Optional['ServiceCache'] = None
    _lock = threading.Lock()

    def __init__(self):
        self._cache: Dict[str, CacheEntry] = {}
        self._cache_lock = threading.Lock()
        logger.info("ğŸ”§ Service cache initialized")

    @classmethod
    def get_instance(cls) -> 'ServiceCache':
        """Get singleton instance (thread-safe)"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = ServiceCache()
        return cls._instance

    def get_or_create(
        self,
        key: str,
        factory: callable,
        ttl: Optional[int] = None,
        force_reload: bool = False
    ) -> Any:
        """
        Get cached service or create new one.

        Args:
            key: Cache key (e.g., 'nlp_service')
            factory: Function to create service if not cached
            ttl: Time to live in seconds (None = infinite)
            force_reload: Force recreation even if cached

        Returns:
            Service instance
        """
        with self._cache_lock:
            # Check if cached and valid
            if not force_reload and key in self._cache:
                entry = self._cache[key]

                # Check TTL
                if ttl is not None:
                    age = (datetime.utcnow() - entry.created_at).total_seconds()
                    if age > ttl:
                        logger.info(f"â™»ï¸  Cache expired for '{key}' (age: {age:.0f}s > {ttl}s)")
                        del self._cache[key]
                    else:
                        # Valid cache hit
                        entry.access_count += 1
                        entry.last_accessed = datetime.utcnow()
                        logger.debug(f"âœ… Cache HIT: '{key}' (accesses: {entry.access_count})")
                        return entry.service
                else:
                    # No TTL, always valid
                    entry.access_count += 1
                    entry.last_accessed = datetime.utcnow()
                    logger.debug(f"âœ… Cache HIT: '{key}' (accesses: {entry.access_count})")
                    return entry.service

            # Cache miss - create new service
            logger.info(f"ğŸ”¨ Cache MISS: Creating '{key}'...")
            service = factory()

            self._cache[key] = CacheEntry(
                service=service,
                created_at=datetime.utcnow(),
                access_count=1,
                last_accessed=datetime.utcnow()
            )

            logger.info(f"âœ… Cached '{key}' (total cached: {len(self._cache)})")
            return service

    def invalidate(self, key: str) -> bool:
        """Remove service from cache"""
        with self._cache_lock:
            if key in self._cache:
                del self._cache[key]
                logger.info(f"ğŸ—‘ï¸  Invalidated cache: '{key}'")
                return True
            return False

    def clear(self) -> int:
        """Clear entire cache"""
        with self._cache_lock:
            count = len(self._cache)
            self._cache.clear()
            logger.info(f"ğŸ—‘ï¸  Cleared cache ({count} entries removed)")
            return count

    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        with self._cache_lock:
            return {
                'total_entries': len(self._cache),
                'entries': {
                    key: {
                        'created_at': entry.created_at.isoformat(),
                        'access_count': entry.access_count,
                        'last_accessed': entry.last_accessed.isoformat(),
                        'age_seconds': (datetime.utcnow() - entry.created_at).total_seconds()
                    }
                    for key, entry in self._cache.items()
                }
            }


# Global instance
_cache = ServiceCache.get_instance()


def get_service_cache() -> ServiceCache:
    """Get global service cache instance"""
    return _cache
```

---

### Step 2: Update API to Use Cached Services

**File:** `hienfeld_api/app.py` (MODIFY)

**Before:**
```python
def _run_analysis_job(...):
    # âŒ Creates new instances every time
    nlp_service = NLPService(config)
    embeddings_service = create_embeddings_service(...)
    tfidf_service = DocumentSimilarityService(config)
```

**After:**
```python
from hienfeld.services.service_cache import get_service_cache

def _run_analysis_job(...):
    cache = get_service_cache()

    # âœ… Get or create cached NLP service
    nlp_service = cache.get_or_create(
        f'nlp_service_{mode.value}',
        lambda: NLPService(config),
        ttl=3600  # 1 hour cache
    )

    # âœ… Get or create cached embeddings service
    if config.semantic.enable_embeddings:
        embeddings_service = cache.get_or_create(
            f'embeddings_{config.semantic.embedding_model}',
            lambda: create_embeddings_service(config.semantic.embedding_model),
            ttl=3600
        )

    # âœ… TF-IDF trained once per corpus (cache by policy files hash)
    # Note: TF-IDF needs to be trained on specific corpus, so cache key includes content hash
    # For now, create fresh but reuse if same corpus
```

**Detailed Changes:**

1. **Import cache:**
```python
# After line 33
from hienfeld.services.service_cache import get_service_cache
```

2. **Cache NLP service (line ~260):**
```python
# BEFORE (line 256-267):
nlp_service_for_naming = None
if config.semantic.enabled and config.semantic.enable_nlp:
    try:
        from hienfeld.services.nlp_service import NLPService
        nlp_service_for_naming = NLPService(config)
        if nlp_service_for_naming.is_available:
            logger.info("âœ… NLP service loaded for semantic cluster naming")
        else:
            nlp_service_for_naming = None
    except Exception as e:
        logger.debug(f"NLP service not available for cluster naming: {e}")
        nlp_service_for_naming = None

# AFTER:
nlp_service_for_naming = None
if config.semantic.enabled and config.semantic.enable_nlp:
    try:
        from hienfeld.services.nlp_service import NLPService
        cache = get_service_cache()

        # Try to get cached NLP service
        nlp_service_for_naming = cache.get_or_create(
            'nlp_service_nl_core_news_md',
            lambda: NLPService(config),
            ttl=None  # Cache indefinitely (model doesn't change)
        )

        if nlp_service_for_naming.is_available:
            logger.info("âœ… NLP service loaded (cached)")
        else:
            nlp_service_for_naming = None
    except Exception as e:
        logger.debug(f"NLP service not available for cluster naming: {e}")
        nlp_service_for_naming = None
```

3. **Cache embeddings service (line ~348):**
```python
# BEFORE (line 345-372):
if config.semantic.enable_embeddings:
    _update_job(job, progress=14, message="ğŸ§  Embeddings laden...")
    try:
        embeddings_service = create_embeddings_service(
            model_name=config.semantic.embedding_model
        )
        # ... rest of code

# AFTER:
if config.semantic.enable_embeddings:
    _update_job(job, progress=14, message="ğŸ§  Embeddings laden...")
    try:
        cache = get_service_cache()

        # Get or create cached embeddings service
        embeddings_service = cache.get_or_create(
            f'embeddings_{config.semantic.embedding_model}',
            lambda: create_embeddings_service(
                model_name=config.semantic.embedding_model
            ),
            ttl=None  # Cache indefinitely
        )

        logger.info(f"âœ… Embeddings service loaded (model: {config.semantic.embedding_model})")

        # Vector store creation (lightweight, not worth caching)
        _update_job(job, progress=16, message="ğŸ“Š Vector store initialiseren...")
        vector_store = create_vector_store(
            method=config.ai.vector_store_type or "faiss",
            embedding_dim=getattr(embeddings_service, "embedding_dim", 384),
        )
        # ... rest of code
```

---

### Step 3: Add Cache Management Endpoint

**File:** `hienfeld_api/app.py` (ADD)

Add new endpoints for cache management:

```python
# After line 827 (after healthcheck endpoint)

@app.get("/api/cache/stats")
async def get_cache_stats() -> Dict[str, Any]:
    """Get service cache statistics (for debugging/monitoring)"""
    cache = get_service_cache()
    return cache.get_stats()


@app.post("/api/cache/clear")
async def clear_cache() -> Dict[str, Any]:
    """Clear service cache (force reload on next request)"""
    cache = get_service_cache()
    count = cache.clear()
    return {
        "status": "ok",
        "message": f"Cleared {count} cached services",
        "cleared_count": count
    }


@app.delete("/api/cache/{key}")
async def invalidate_cache(key: str) -> Dict[str, Any]:
    """Invalidate specific cache entry"""
    cache = get_service_cache()
    success = cache.invalidate(key)
    if success:
        return {"status": "ok", "message": f"Invalidated '{key}'"}
    else:
        raise HTTPException(status_code=404, detail=f"Cache key '{key}' not found")
```

---

### Expected Performance Improvement

**Before (no caching):**
```
Request 1: 60s (model loading) + 240s (analysis) = 300s total
Request 2: 60s (model loading) + 240s (analysis) = 300s total
Request 3: 60s (model loading) + 240s (analysis) = 300s total

Total for 3 requests: 900s (15 minutes)
Memory: 3Ã— models = ~900MB
```

**After (with caching):**
```
Request 1: 60s (model loading) + 240s (analysis) = 300s total
Request 2: 2s (cache hit) + 240s (analysis) = 242s total âœ…
Request 3: 2s (cache hit) + 240s (analysis) = 242s total âœ…

Total for 3 requests: 784s (13 minutes) â†’ 15% faster
First request after cache: 242s vs 300s â†’ 20% faster!
Memory: 1Ã— models = ~300MB â†’ 67% less memory
```

**User Experience:**
- First analysis of the day: Same speed
- Subsequent analyses: **20% faster startup** (no model loading wait)
- Server memory: **67% reduction** (shared models)

---

### Testing Plan

**Step 1: Unit Test**
```python
# tests/test_service_cache.py
def test_service_cache_basic():
    cache = ServiceCache.get_instance()
    cache.clear()

    # First call creates service
    call_count = 0
    def factory():
        nonlocal call_count
        call_count += 1
        return {"model": "test"}

    service1 = cache.get_or_create('test', factory)
    assert call_count == 1

    # Second call uses cache
    service2 = cache.get_or_create('test', factory)
    assert call_count == 1  # Factory not called again
    assert service1 is service2  # Same instance


def test_service_cache_ttl():
    cache = ServiceCache.get_instance()
    cache.clear()

    service1 = cache.get_or_create('test', lambda: {}, ttl=1)

    import time
    time.sleep(2)

    service2 = cache.get_or_create('test', lambda: {}, ttl=1)
    assert service1 is not service2  # Expired, new instance
```

**Step 2: Integration Test**
```bash
# Terminal 1: Start server
uvicorn hienfeld_api.app:app --reload --port 8000

# Terminal 2: Check cache stats (should be empty)
curl http://localhost:8000/api/cache/stats

# Terminal 3: Start analysis
curl -X POST http://localhost:8000/api/analyze \
  -F "policy_file=@test.xlsx" \
  -F "analysis_mode=balanced"

# Monitor logs - should see "Cache MISS" first time
# Start another analysis - should see "Cache HIT"

# Check cache stats again
curl http://localhost:8000/api/cache/stats
# Should show cached services with access counts
```

**Expected Output:**
```json
{
  "total_entries": 2,
  "entries": {
    "nlp_service_nl_core_news_md": {
      "created_at": "2025-01-15T10:30:00",
      "access_count": 3,
      "last_accessed": "2025-01-15T10:35:00",
      "age_seconds": 300
    },
    "embeddings_all-MiniLM-L6-v2": {
      "created_at": "2025-01-15T10:30:05",
      "access_count": 3,
      "last_accessed": "2025-01-15T10:35:00",
      "age_seconds": 295
    }
  }
}
```

---

## ğŸ“‹ Phase 2: Redis Job Storage

### Overview

Replace in-memory `JOBS` dict with Redis for persistent state.

**Benefits:**
- âœ… Jobs persist across restarts
- âœ… Can scale horizontally (multiple API instances)
- âœ… Can resume failed jobs
- âœ… Audit trail

---

### Step 1: Install Redis

**Local Development:**
```bash
# macOS
brew install redis
brew services start redis

# Verify
redis-cli ping
# Should return: PONG
```

**Docker (Alternative):**
```bash
docker run -d -p 6379:6379 --name hienfeld-redis redis:7-alpine
```

**Python Dependency:**
```bash
pip install redis python-redis-lock
```

Add to `requirements.txt`:
```
redis>=5.0.0
python-redis-lock>=4.0.0
```

---

### Step 2: Create Redis Job Store

**File:** `hienfeld/infrastructure/redis_job_store.py` (NEW)

```python
"""
Redis-based job storage for persistent state across server restarts.
"""

from __future__ import annotations

import json
import pickle
from typing import Dict, List, Optional, Any
from dataclasses import asdict
from datetime import datetime

import redis
from redis.lock import Lock

from hienfeld.logging_config import get_logger

logger = get_logger('redis_job_store')


class RedisJobStore:
    """
    Persistent job storage using Redis.

    Features:
    - Jobs persist across server restarts
    - Thread-safe with Redis locks
    - TTL for automatic cleanup
    - Can scale horizontally

    Usage:
        store = RedisJobStore(redis_url="redis://localhost:6379/0")

        # Save job
        store.save_job(job)

        # Get job
        job = store.get_job(job_id)

        # Update progress
        store.update_job(job_id, progress=50, message="Processing...")
    """

    def __init__(
        self,
        redis_url: str = "redis://localhost:6379/0",
        key_prefix: str = "hienfeld:job:",
        job_ttl: int = 86400,  # 24 hours
    ):
        """
        Initialize Redis job store.

        Args:
            redis_url: Redis connection URL
            key_prefix: Prefix for all job keys
            job_ttl: Time to live for jobs in seconds (default: 24h)
        """
        self.client = redis.from_url(redis_url, decode_responses=False)
        self.key_prefix = key_prefix
        self.job_ttl = job_ttl

        # Test connection
        try:
            self.client.ping()
            logger.info(f"âœ… Connected to Redis: {redis_url}")
        except redis.ConnectionError as e:
            logger.error(f"âŒ Failed to connect to Redis: {e}")
            raise

    def _job_key(self, job_id: str) -> str:
        """Get Redis key for job"""
        return f"{self.key_prefix}{job_id}"

    def save_job(self, job: Any) -> None:
        """
        Save job to Redis.

        Args:
            job: AnalysisJob instance
        """
        key = self._job_key(job.id)

        # Serialize job (using pickle for complex objects)
        job_data = pickle.dumps(job)

        # Save with TTL
        self.client.setex(key, self.job_ttl, job_data)
        logger.debug(f"ğŸ’¾ Saved job {job.id[:8]} to Redis")

    def get_job(self, job_id: str) -> Optional[Any]:
        """
        Get job from Redis.

        Args:
            job_id: Job ID

        Returns:
            AnalysisJob instance or None if not found
        """
        key = self._job_key(job_id)
        job_data = self.client.get(key)

        if job_data is None:
            logger.debug(f"âŒ Job {job_id[:8]} not found in Redis")
            return None

        # Deserialize
        job = pickle.loads(job_data)
        logger.debug(f"âœ… Retrieved job {job_id[:8]} from Redis")
        return job

    def update_job(
        self,
        job_id: str,
        **updates
    ) -> bool:
        """
        Update job fields atomically.

        Args:
            job_id: Job ID
            **updates: Fields to update (e.g., progress=50, status='completed')

        Returns:
            True if updated, False if job not found
        """
        # Use Redis lock for atomic update
        lock_key = f"{self._job_key(job_id)}:lock"

        with Lock(self.client, lock_key, timeout=5):
            job = self.get_job(job_id)
            if job is None:
                return False

            # Update fields
            for key, value in updates.items():
                if hasattr(job, key):
                    setattr(job, key, value)

            # Save back
            self.save_job(job)
            logger.debug(f"ğŸ”„ Updated job {job_id[:8]}: {updates}")
            return True

    def delete_job(self, job_id: str) -> bool:
        """Delete job from Redis"""
        key = self._job_key(job_id)
        deleted = self.client.delete(key)
        if deleted:
            logger.info(f"ğŸ—‘ï¸  Deleted job {job_id[:8]} from Redis")
        return bool(deleted)

    def list_jobs(
        self,
        status: Optional[str] = None,
        limit: int = 100
    ) -> List[Any]:
        """
        List all jobs (or filter by status).

        Args:
            status: Filter by status (e.g., 'running', 'completed')
            limit: Max jobs to return

        Returns:
            List of AnalysisJob instances
        """
        # Scan for all job keys
        pattern = f"{self.key_prefix}*"
        jobs = []

        for key in self.client.scan_iter(match=pattern, count=100):
            if len(jobs) >= limit:
                break

            job_data = self.client.get(key)
            if job_data:
                job = pickle.loads(job_data)

                # Filter by status if requested
                if status is None or job.status == status:
                    jobs.append(job)

        return jobs

    def cleanup_old_jobs(self, max_age_hours: int = 24) -> int:
        """
        Clean up old completed/failed jobs.

        Args:
            max_age_hours: Delete jobs older than this

        Returns:
            Number of jobs deleted
        """
        from datetime import timedelta

        cutoff = datetime.utcnow() - timedelta(hours=max_age_hours)
        deleted = 0

        for job in self.list_jobs(limit=1000):
            if job.created_at < cutoff and job.status in ['completed', 'failed']:
                if self.delete_job(job.id):
                    deleted += 1

        logger.info(f"ğŸ—‘ï¸  Cleaned up {deleted} old jobs (older than {max_age_hours}h)")
        return deleted

    def get_stats(self) -> Dict[str, Any]:
        """Get storage statistics"""
        pattern = f"{self.key_prefix}*"
        total = sum(1 for _ in self.client.scan_iter(match=pattern, count=100))

        jobs = self.list_jobs(limit=1000)
        by_status = {}
        for job in jobs:
            by_status[job.status] = by_status.get(job.status, 0) + 1

        return {
            'total_jobs': total,
            'by_status': by_status,
            'redis_info': {
                'connected': self.client.ping(),
                'db_size': self.client.dbsize(),
            }
        }
```

---

### Step 3: Update API to Use Redis

**File:** `hienfeld_api/app.py` (MODIFY)

**Changes:**

1. **Import Redis store:**
```python
# After line 49
from hienfeld.infrastructure.redis_job_store import RedisJobStore
import os

# Initialize Redis job store (fallback to in-memory if Redis unavailable)
REDIS_URL = os.getenv('REDIS_URL', 'redis://localhost:6379/0')
USE_REDIS = os.getenv('USE_REDIS', 'true').lower() in ('true', '1', 'yes')

if USE_REDIS:
    try:
        job_store = RedisJobStore(redis_url=REDIS_URL, job_ttl=86400)
        logger.info("âœ… Using Redis for job storage")
    except Exception as e:
        logger.warning(f"âš ï¸ Redis unavailable, falling back to in-memory: {e}")
        job_store = None
        JOBS: Dict[str, AnalysisJob] = {}
else:
    logger.info("â„¹ï¸  Using in-memory job storage (USE_REDIS=false)")
    job_store = None
    JOBS: Dict[str, AnalysisJob] = {}
```

2. **Helper functions for job storage:**
```python
# After line 136

def _get_job(job_id: str) -> Optional[AnalysisJob]:
    """Get job from Redis or in-memory store"""
    if job_store:
        return job_store.get_job(job_id)
    else:
        return JOBS.get(job_id)


def _save_job(job: AnalysisJob) -> None:
    """Save job to Redis or in-memory store"""
    if job_store:
        job_store.save_job(job)
    else:
        JOBS[job.id] = job


def _update_job_storage(job_id: str, **updates) -> bool:
    """Update job in storage"""
    if job_store:
        return job_store.update_job(job_id, **updates)
    else:
        job = JOBS.get(job_id)
        if job:
            for key, value in updates.items():
                if hasattr(job, key):
                    setattr(job, key, value)
            return True
        return False
```

3. **Update all `JOBS.get()` calls to `_get_job()`:**
```python
# Find and replace throughout file:
# JOBS.get(job_id) â†’ _get_job(job_id)
# JOBS[job_id] = job â†’ _save_job(job)
```

4. **Add Redis management endpoints:**
```python
# After line 827

@app.get("/api/jobs/list")
async def list_jobs(status: Optional[str] = None, limit: int = 100) -> Dict[str, Any]:
    """List all jobs (requires Redis)"""
    if not job_store:
        raise HTTPException(status_code=503, detail="Redis not available")

    jobs = job_store.list_jobs(status=status, limit=limit)
    return {
        'total': len(jobs),
        'jobs': [
            {
                'job_id': job.id,
                'status': job.status,
                'progress': job.progress,
                'created_at': job.created_at.isoformat(),
                'message': job.status_message
            }
            for job in jobs
        ]
    }


@app.get("/api/jobs/stats")
async def get_job_stats() -> Dict[str, Any]:
    """Get job storage statistics"""
    if job_store:
        return job_store.get_stats()
    else:
        return {
            'total_jobs': len(JOBS),
            'by_status': {
                status: sum(1 for j in JOBS.values() if j.status == status)
                for status in ['pending', 'running', 'completed', 'failed']
            },
            'storage_type': 'in-memory'
        }


@app.post("/api/jobs/cleanup")
async def cleanup_old_jobs(max_age_hours: int = 24) -> Dict[str, Any]:
    """Clean up old jobs"""
    if not job_store:
        raise HTTPException(status_code=503, detail="Redis not available")

    deleted = job_store.cleanup_old_jobs(max_age_hours=max_age_hours)
    return {
        'status': 'ok',
        'deleted_count': deleted,
        'message': f'Cleaned up {deleted} old jobs'
    }
```

---

### Configuration

**Environment Variables:**
```bash
# .env file
REDIS_URL=redis://localhost:6379/0  # Redis connection URL
USE_REDIS=true                      # Enable Redis (false = in-memory fallback)
REDIS_JOB_TTL=86400                 # Job TTL in seconds (24 hours)
```

**Production:**
```bash
# Azure Redis Cache / AWS ElastiCache
REDIS_URL=rediss://your-redis.cache.windows.net:6380?ssl_cert_reqs=required
```

---

## ğŸ“‹ Phase 3: Enhanced Monitoring

### Overview

Add Prometheus metrics and structured logging for better observability.

---

### Step 1: Add Prometheus Metrics

**Install:**
```bash
pip install prometheus-fastapi-instrumentator
```

**File:** `hienfeld_api/app.py` (MODIFY)

```python
# After line 66
from prometheus_fastapi_instrumentator import Instrumentator

# After app creation (line 66)
# Add Prometheus metrics
instrumentator = Instrumentator(
    should_group_status_codes=False,
    should_ignore_untemplated=True,
    should_respect_env_var=True,
    should_instrument_requests_inprogress=True,
    excluded_handlers=["/metrics"],
    env_var_name="ENABLE_METRICS",
    inprogress_name="http_requests_inprogress",
    inprogress_labels=True,
)

# Instrument the app
instrumentator.instrument(app).expose(app, endpoint="/metrics")

logger.info("ğŸ“Š Prometheus metrics enabled at /metrics")
```

**Custom Metrics:**
```python
# After instrumentator setup
from prometheus_client import Counter, Histogram, Gauge

# Custom metrics
analysis_jobs_total = Counter(
    'hienfeld_analysis_jobs_total',
    'Total number of analysis jobs started',
    ['analysis_mode']
)

analysis_duration_seconds = Histogram(
    'hienfeld_analysis_duration_seconds',
    'Time spent on analysis',
    ['analysis_mode'],
    buckets=[30, 60, 120, 300, 600, 1200, 1800]  # 30s, 1m, 2m, 5m, 10m, 20m, 30m
)

cluster_count = Histogram(
    'hienfeld_cluster_count',
    'Number of clusters created',
    buckets=[10, 50, 100, 250, 500, 1000, 2000]
)

job_queue_depth = Gauge(
    'hienfeld_job_queue_depth',
    'Number of jobs in queue',
    ['status']
)


# Use in endpoints:
@app.post("/api/analyze")
async def start_analysis(...):
    # Increment counter
    analysis_jobs_total.labels(analysis_mode=analysis_mode).inc()

    # ... rest of code
```

---

### Step 2: Structured Logging with Context

**File:** `hienfeld/logging_config.py` (ENHANCE)

Add structured logging with request context:

```python
import contextvars

# Request context
request_id_var = contextvars.ContextVar('request_id', default=None)
job_id_var = contextvars.ContextVar('job_id', default=None)


class StructuredFormatter(logging.Formatter):
    """Formatter with structured context (JSON-compatible)"""

    def format(self, record):
        # Add context
        record.request_id = request_id_var.get()
        record.job_id = job_id_var.get()

        # Format as JSON for production
        return json.dumps({
            'timestamp': datetime.fromtimestamp(record.created).isoformat(),
            'level': record.levelname,
            'logger': record.name,
            'message': record.getMessage(),
            'request_id': record.request_id,
            'job_id': record.job_id,
            'module': record.module,
            'function': record.funcName,
            'line': record.lineno
        })


def set_request_context(request_id: str, job_id: Optional[str] = None):
    """Set request context for logging"""
    request_id_var.set(request_id)
    if job_id:
        job_id_var.set(job_id)
```

**Usage in API:**
```python
@app.post("/api/analyze")
async def start_analysis(...):
    job_id = str(uuid.uuid4())
    set_request_context(request_id=str(uuid.uuid4()), job_id=job_id)

    logger.info("Started analysis", extra={
        'analysis_mode': analysis_mode,
        'file_size': len(policy_bytes)
    })
```

---

## ğŸ“‹ Phase 4: Celery Task Queue (Optional)

### Overview

Replace FastAPI BackgroundTasks with Celery for:
- Distributed processing
- Retry logic
- Priority queues
- Better monitoring

**Note:** This is the most complex change and can be deferred if not needed immediately.

---

### Quick Reference

**Dependencies:**
```bash
pip install celery[redis]
```

**File:** `hienfeld/tasks/celery_app.py` (NEW)

```python
from celery import Celery

celery_app = Celery(
    'hienfeld',
    broker='redis://localhost:6379/1',
    backend='redis://localhost:6379/1'
)

@celery_app.task(bind=True, max_retries=3)
def run_analysis_task(self, job_id, policy_bytes, ...):
    """Analysis task with retry logic"""
    try:
        _run_analysis_job(job_id, policy_bytes, ...)
    except Exception as exc:
        # Retry with exponential backoff
        raise self.retry(exc=exc, countdown=2 ** self.request.retries)
```

**More details available on request - this is a larger change.**

---

## ğŸš€ Deployment Timeline

### Week 1: Model Caching
- Day 1-2: Implement `ServiceCache`
- Day 3: Integrate into API
- Day 4: Test and benchmark
- Day 5: Deploy to production

**Expected:** 20% faster repeat analyses, 67% less memory

---

### Week 2: Redis Job Storage
- Day 1-2: Implement `RedisJobStore`
- Day 3: Update API
- Day 4: Test failover scenarios
- Day 5: Deploy with Redis

**Expected:** Jobs persist, horizontal scaling enabled

---

### Week 2-3: Monitoring
- Day 1: Add Prometheus metrics
- Day 2: Set up dashboards
- Day 3: Add alerting

**Expected:** Better visibility, faster debugging

---

### Week 3+ (Optional): Celery
- Only if needed for:
  - High concurrency (>10 simultaneous analyses)
  - Advanced retry logic
  - Priority queues

---

## âœ… Success Criteria

**After Phase 1 (Model Caching):**
- [ ] First analysis: Same speed
- [ ] Second+ analyses: 20% faster
- [ ] Memory usage: 67% lower
- [ ] Cache hit rate: >80%

**After Phase 2 (Redis):**
- [ ] Jobs survive restart
- [ ] Can run multiple API instances
- [ ] No lost job data

**After Phase 3 (Monitoring):**
- [ ] Metrics visible at `/metrics`
- [ ] Can track performance trends
- [ ] Alerts on failures

---

## ğŸ“ Next Steps

1. **Review this plan** - Aanpassingen?
2. **Start Phase 1** - Model caching (hoogste impact)
3. **Test thoroughly** - Benchmark before/after
4. **Deploy incrementally** - One phase at a time

**Klaar om te beginnen met Phase 1: Model Caching?** ğŸš€
